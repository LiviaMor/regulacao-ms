"""
RAG INTEGRATION - MS-REGULACAO - VERSÃƒO FOCADA
IntegraÃ§Ã£o do Pipeline RAG focado com LLMs (Llama 3, GPT-4, Claude, etc.)
Seguindo hierarquia SUS: UPA -> Regional -> ReferÃªncia
"""

import sys
import os
import json
import logging
from typing import Dict, Any, Optional
import requests
from datetime import datetime

# Adicionar path para pipeline RAG focado
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from pipeline_hospitais_goias_rag import (
    gerar_contexto_rag_llama, 
    gerar_prompt_completo_llama,
    pipeline_rag
)

logger = logging.getLogger(__name__)

class RAGRegulacaoMedica:
    """
    Classe para integraÃ§Ã£o RAG focada com diferentes LLMs
    Implementa lÃ³gica de peneira: Especialidade -> Complexidade -> Localidade
    """
    
    def __init__(self):
        self.llm_configs = {
            "ollama": {
                "url": os.getenv("OLLAMA_URL", "http://localhost:11434"),
                "model": "llama3",
                "endpoint": "/api/generate"
            },
            "openai": {
                "url": "https://api.openai.com/v1/chat/completions",
                "model": "gpt-4",
                "api_key": os.getenv("OPENAI_API_KEY")
            },
            "anthropic": {
                "url": "https://api.anthropic.com/v1/messages",
                "model": "claude-3-sonnet-20240229",
                "api_key": os.getenv("ANTHROPIC_API_KEY")
            }
        }
    
    def processar_com_llm(self, dados_paciente: Dict[str, Any], 
                         llm_provider: str = "ollama",
                         resultado_biobert: str = None) -> Dict[str, Any]:
        """
        Processa regulaÃ§Ã£o usando LLM com contexto RAG focado
        
        Args:
            dados_paciente: Dados do paciente
            llm_provider: Provedor do LLM (ollama, openai, anthropic)
            resultado_biobert: Resultado da anÃ¡lise BioBERT (opcional)
            
        Returns:
            Resposta processada do LLM
        """
        
        try:
            # 1. Gerar prompt completo com contexto RAG focado
            prompt_completo = gerar_prompt_completo_llama(dados_paciente, resultado_biobert)
            
            logger.info(f"ğŸ¤– Processando com {llm_provider.upper()}: {dados_paciente.get('protocolo')}")
            
            # 2. Chamar LLM especÃ­fico
            if llm_provider == "ollama":
                resposta_llm = self._chamar_ollama(prompt_completo)
            elif llm_provider == "openai":
                resposta_llm = self._chamar_openai(prompt_completo)
            elif llm_provider == "anthropic":
                resposta_llm = self._chamar_anthropic(prompt_completo)
            else:
                raise ValueError(f"Provedor LLM nÃ£o suportado: {llm_provider}")
            
            # 3. Processar e validar resposta
            resposta_processada = self._processar_resposta_llm_focada(resposta_llm)
            
            # 4. Adicionar metadados RAG
            resposta_processada.update({
                "rag_metadata": {
                    "llm_provider": llm_provider,
                    "prompt_size": len(prompt_completo),
                    "pipeline_version": "RAG_Focado_v1.0",
                    "hierarquia_sus_aplicada": True,
                    "filtro_peneira_usado": True,
                    "processado_em": datetime.utcnow().isoformat()
                }
            })
            
            logger.info(f"âœ… {llm_provider.upper()} processou: {resposta_processada['hospital_escolhido']}")
            
            return resposta_processada
            
        except Exception as e:
            logger.error(f"âŒ Erro no processamento RAG com {llm_provider}: {e}")
            
            # Fallback para pipeline tradicional
            return self._fallback_pipeline_tradicional(dados_paciente)
    
    def _processar_resposta_llm_focada(self, resposta_llm: str) -> Dict[str, Any]:
        """
        Processa e valida a resposta do LLM (versÃ£o focada)
        """
        try:
            # Tentar extrair JSON da resposta
            if "```json" in resposta_llm:
                json_start = resposta_llm.find("```json") + 7
                json_end = resposta_llm.find("```", json_start)
                json_str = resposta_llm[json_start:json_end].strip()
            elif "{" in resposta_llm and "}" in resposta_llm:
                # Extrair JSON mesmo sem marcadores
                json_start = resposta_llm.find("{")
                json_end = resposta_llm.rfind("}") + 1
                json_str = resposta_llm[json_start:json_end]
            else:
                json_str = resposta_llm.strip()
            
            resposta = json.loads(json_str)
            
            # Validar campos obrigatÃ³rios
            campos_obrigatorios = ["hospital_escolhido", "justificativa"]
            for campo in campos_obrigatorios:
                if campo not in resposta:
                    raise ValueError(f"Campo obrigatÃ³rio '{campo}' nÃ£o encontrado na resposta")
            
            # Validar se hospital existe na base
            hospital_escolhido = resposta["hospital_escolhido"]
            hospital_valido = any(h.nome == hospital_escolhido for h in pipeline_rag.hospitais)
            
            if not hospital_valido:
                logger.warning(f"Hospital '{hospital_escolhido}' nÃ£o encontrado na base. Usando fallback.")
                resposta["hospital_escolhido"] = "HOSPITAL ESTADUAL DR ALBERTO RASSI HGG"
                resposta["justificativa"] += " [FALLBACK: Hospital original nÃ£o encontrado]"
            
            # Converter para formato padrÃ£o do sistema
            resposta_padrao = {
                "hospital_escolhido": resposta["hospital_escolhido"],
                "justificativa_tecnica": resposta["justificativa"],
                "score_adequacao": resposta.get("nivel_sus", 8),
                "tipo_transporte": "USB",  # PadrÃ£o
                "observacoes_clinicas": f"NÃ­vel SUS: {resposta.get('nivel_sus', 'N/A')}",
                "restricoes_verificadas": resposta.get("restricoes_respeitadas", []),
                "processado_em": datetime.utcnow().isoformat(),
                "fonte": "LLM_RAG_Focado",
                "validado": True
            }
            
            # Ajustar transporte baseado na urgÃªncia
            if any(palavra in resposta["justificativa"].lower() for palavra in ["trauma", "urgente", "emergÃªncia"]):
                resposta_padrao["tipo_transporte"] = "USA"
            
            return resposta_padrao
            
        except Exception as e:
            logger.error(f"Erro ao processar resposta do LLM: {e}")
            
            # Fallback em caso de erro
            return {
                "hospital_escolhido": "HOSPITAL ESTADUAL DR ALBERTO RASSI HGG",
                "justificativa_tecnica": f"Erro no processamento da resposta do LLM: {str(e)}. Usando hospital de referÃªncia como fallback.",
                "score_adequacao": 5,
                "tipo_transporte": "USB",
                "observacoes_clinicas": "AnÃ¡lise manual necessÃ¡ria devido a erro no LLM",
                "restricoes_verificadas": [],
                "processado_em": datetime.utcnow().isoformat(),
                "fonte": "FALLBACK_RAG_Focado",
                "validado": False,
                "erro": str(e)
            }
    
    def _chamar_ollama(self, prompt: str) -> str:
        """Chama Ollama (Llama 3 local)"""
        
        config = self.llm_configs["ollama"]
        
        payload = {
            "model": config["model"],
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,  # Baixa temperatura para consistÃªncia mÃ©dica
                "top_p": 0.9,
                "max_tokens": 1000
            }
        }
        
        response = requests.post(
            f"{config['url']}{config['endpoint']}", 
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        return response.json().get("response", "")
    
    def _chamar_openai(self, prompt: str) -> str:
        """Chama OpenAI GPT-4"""
        
        config = self.llm_configs["openai"]
        
        if not config.get("api_key"):
            raise ValueError("OPENAI_API_KEY nÃ£o configurada")
        
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [
                {
                    "role": "system", 
                    "content": "VocÃª Ã© um especialista em regulaÃ§Ã£o mÃ©dica do SUS de GoiÃ¡s. Responda sempre em JSON conforme solicitado."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": 0.1,
            "max_tokens": 1000
        }
        
        response = requests.post(
            config["url"], 
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        return response.json()["choices"][0]["message"]["content"]
    
    def _chamar_anthropic(self, prompt: str) -> str:
        """Chama Anthropic Claude"""
        
        config = self.llm_configs["anthropic"]
        
        if not config.get("api_key"):
            raise ValueError("ANTHROPIC_API_KEY nÃ£o configurada")
        
        headers = {
            "x-api-key": config["api_key"],
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": 1000,
            "temperature": 0.1,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        response = requests.post(
            config["url"], 
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        return response.json()["content"][0]["text"]
    
    def _fallback_pipeline_tradicional(self, dados_paciente: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para pipeline tradicional em caso de erro no LLM"""
        
        # Usar pipeline RAG focado como fallback (sem LLM)
        especialidade = dados_paciente.get('especialidade', 'CLINICA_MEDICA')
        cid = dados_paciente.get('cid', '')
        cidade = dados_paciente.get('cidade_origem', 'GOIANIA')
        
        # Aplicar filtro de peneira
        hospitais_filtrados = pipeline_rag.aplicar_filtro_peneira(especialidade, cid, cidade)
        
        if hospitais_filtrados:
            hospital_escolhido = hospitais_filtrados[0]  # Primeiro da lista (melhor ranqueado)
            justificativa = f"Hospital selecionado via pipeline focado: {hospital_escolhido.observacoes}"
        else:
            hospital_escolhido = None
            justificativa = "Nenhum hospital adequado encontrado, usando referÃªncia geral"
        
        return {
            "hospital_escolhido": hospital_escolhido.nome if hospital_escolhido else "HOSPITAL ESTADUAL DR ALBERTO RASSI HGG",
            "justificativa_tecnica": justificativa,
            "score_adequacao": 7,
            "tipo_transporte": "USB",
            "observacoes_clinicas": "Processado via pipeline focado (fallback)",
            "restricoes_verificadas": [],
            "processado_em": datetime.utcnow().isoformat(),
            "fonte": "FALLBACK_Pipeline_Focado",
            "validado": True,
            "rag_metadata": {
                "llm_provider": "fallback",
                "pipeline_version": "RAG_Focado_v1.0",
                "hierarquia_sus_aplicada": True
            }
        }
    
    def testar_integracao_llm(self, llm_provider: str = "ollama") -> bool:
        """
        Testa integraÃ§Ã£o com LLM especÃ­fico usando pipeline focado
        """
        
        dados_teste = {
            "protocolo": "RAG-TEST-001",
            "especialidade": "ORTOPEDIA",
            "cid": "M54.5",
            "cid_desc": "Dor lombar",
            "cidade_origem": "ANAPOLIS",
            "prontuario_texto": "Paciente com dor lombar crÃ´nica, sem trauma",
            "historico_paciente": "Dor recorrente hÃ¡ 6 meses",
            "prioridade_descricao": "Normal"
        }
        
        try:
            resultado = self.processar_com_llm(dados_teste, llm_provider)
            
            # Verificar se resultado Ã© vÃ¡lido
            hospital_escolhido = resultado.get("hospital_escolhido", "")
            
            # Teste especÃ­fico: dor lombar NÃƒO deve ir para HUGO
            hugo_evitado = "HUGO" not in hospital_escolhido
            
            # Verificar se tem justificativa
            tem_justificativa = bool(resultado.get("justificativa_tecnica"))
            
            if hugo_evitado and tem_justificativa:
                logger.info(f"âœ… Teste {llm_provider.upper()} passou: {hospital_escolhido}")
                logger.info(f"âœ… HUGO evitado corretamente para dor lombar")
                return True
            else:
                logger.warning(f"âš ï¸ Teste {llm_provider.upper()} falhou:")
                logger.warning(f"   Hospital: {hospital_escolhido}")
                logger.warning(f"   HUGO evitado: {hugo_evitado}")
                logger.warning(f"   Tem justificativa: {tem_justificativa}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Teste {llm_provider.upper()} falhou: {e}")
            return False


# InstÃ¢ncia global para uso nos microserviÃ§os
rag_regulacao = RAGRegulacaoMedica()

def processar_regulacao_rag(dados_paciente: Dict[str, Any], 
                           llm_provider: str = "ollama",
                           resultado_biobert: str = None) -> Dict[str, Any]:
    """
    FunÃ§Ã£o principal para processamento RAG focado
    
    Args:
        dados_paciente: Dados do paciente
        llm_provider: Provedor LLM (ollama, openai, anthropic)
        resultado_biobert: Resultado da anÃ¡lise BioBERT (opcional)
        
    Returns:
        Resultado processado pelo LLM com contexto RAG focado
    """
    
    return rag_regulacao.processar_com_llm(dados_paciente, llm_provider, resultado_biobert)

def testar_rag_integration() -> Dict[str, bool]:
    """
    Testa integraÃ§Ã£o RAG com todos os provedores disponÃ­veis
    
    Returns:
        Dict com status de cada provedor
    """
    
    resultados = {}
    
    # Testar Ollama (local)
    resultados["ollama"] = rag_regulacao.testar_integracao_llm("ollama")
    
    # Testar OpenAI (se API key disponÃ­vel)
    if os.getenv("OPENAI_API_KEY"):
        resultados["openai"] = rag_regulacao.testar_integracao_llm("openai")
    else:
        resultados["openai"] = False
        logger.info("âš ï¸ OpenAI API Key nÃ£o configurada")
    
    # Testar Anthropic (se API key disponÃ­vel)
    if os.getenv("ANTHROPIC_API_KEY"):
        resultados["anthropic"] = rag_regulacao.testar_integracao_llm("anthropic")
    else:
        resultados["anthropic"] = False
        logger.info("âš ï¸ Anthropic API Key nÃ£o configurada")
    
    return resultados


if __name__ == "__main__":
    print("ğŸ¤– TESTE DE INTEGRAÃ‡ÃƒO RAG FOCADO - REGULAÃ‡ÃƒO MÃ‰DICA")
    print("=" * 60)
    
    # Testar todos os provedores
    resultados = testar_rag_integration()
    
    print("\nğŸ“Š RESULTADOS DOS TESTES:")
    for provedor, status in resultados.items():
        emoji = "âœ…" if status else "âŒ"
        print(f"{emoji} {provedor.upper()}: {'FUNCIONANDO' if status else 'INDISPONÃVEL'}")
    
    # Teste detalhado com pipeline focado
    print("\nğŸ¥ TESTE DETALHADO - PIPELINE FOCADO:")
    
    casos_teste = [
        {
            "nome": "Dor Lombar - AnÃ¡polis (deve evitar HUGO, priorizar regional)",
            "dados": {
                "protocolo": "RAG-DEMO-001",
                "especialidade": "ORTOPEDIA",
                "cid": "M54.5",
                "cidade_origem": "ANAPOLIS",
                "prontuario_texto": "Dor lombar crÃ´nica hÃ¡ 6 meses, sem trauma"
            }
        },
        {
            "nome": "Trauma Craniano - GoiÃ¢nia (deve priorizar HUGO/HUGOL)",
            "dados": {
                "protocolo": "RAG-DEMO-002",
                "especialidade": "NEUROCIRURGIA",
                "cid": "S06.9",
                "cidade_origem": "GOIANIA",
                "prontuario_texto": "Trauma craniano grave, acidente de trÃ¢nsito"
            }
        },
        {
            "nome": "InfecÃ§Ã£o - Formosa (deve priorizar HDT)",
            "dados": {
                "protocolo": "RAG-DEMO-003",
                "especialidade": "INFECTOLOGIA",
                "cid": "A15.0",
                "cidade_origem": "FORMOSA",
                "prontuario_texto": "Suspeita de tuberculose pulmonar"
            }
        }
    ]
    
    for caso in casos_teste:
        print(f"\nğŸ“‹ {caso['nome']}")
        
        # Testar apenas contexto (sem LLM)
        contexto = gerar_contexto_rag_llama(
            caso['dados']['especialidade'],
            caso['dados']['cid'],
            caso['dados']['cidade_origem']
        )
        
        hospitais_contexto = json.loads(contexto)
        if hospitais_contexto:
            primeiro_hospital = hospitais_contexto[0]['hospital']
            print(f"ğŸ¥ Primeiro hospital sugerido: {primeiro_hospital}")
            
            # VerificaÃ§Ãµes especÃ­ficas
            if caso['dados']['cid'] == "M54.5" and "HUGO" not in primeiro_hospital:
                print("âœ… CORRETO: Dor lombar nÃ£o foi para HUGO")
            elif caso['dados']['cid'] == "S06.9" and ("HUGO" in primeiro_hospital or "HUGOL" in primeiro_hospital):
                print("âœ… CORRETO: Trauma foi para hospital especializado")
            elif caso['dados']['cid'] == "A15.0" and "HDT" in primeiro_hospital:
                print("âœ… CORRETO: InfecÃ§Ã£o foi para HDT")
        else:
            print("âŒ Nenhum hospital encontrado")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ PIPELINE RAG FOCADO IMPLEMENTADO!")
    print("ğŸ”„ LÃ³gica de peneira: Especialidade -> Complexidade -> Localidade")
    print("ğŸ¥ Hierarquia SUS: UPA (1) -> Regional (2) -> ReferÃªncia (3)")
    print("ğŸ¤– Pronto para Llama 3, GPT-4, Claude com contexto otimizado")
    print("=" * 60)